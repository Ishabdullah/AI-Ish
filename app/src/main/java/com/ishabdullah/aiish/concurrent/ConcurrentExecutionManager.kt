/*
 * Copyright (c) 2025 Ismail Abdullah. All rights reserved.
 *
 * This software and all associated files are the exclusive property of Ismail Abdullah.
 * Unauthorized use, copying, modification, or distribution is strictly prohibited.
 *
 * Contact: ismail.t.abdullah@gmail.com
 */

package com.ishabdullah.aiish.concurrent

import android.content.Context
import android.graphics.Bitmap
import com.ishabdullah.aiish.device.DeviceAllocationManager
import com.ishabdullah.aiish.embedding.EmbeddingManager
import com.ishabdullah.aiish.ml.LLMInferenceEngine
import com.ishabdullah.aiish.vision.VisionManager
import kotlinx.coroutines.*
import kotlinx.coroutines.flow.Flow
import kotlinx.coroutines.flow.MutableStateFlow
import kotlinx.coroutines.flow.StateFlow
import kotlinx.coroutines.flow.asStateFlow
import timber.log.Timber
import java.io.File

/**
 * ConcurrentExecutionManager - Orchestrates parallel AI model execution
 *
 * Samsung S24 Ultra Concurrent Execution Architecture:
 * â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 * â”‚ NPU (Hexagon v81 - 45 TOPS INT8)                             â”‚
 * â”‚ â”œâ”€ Mistral-7B Prefill (async, 15-20ms)                       â”‚
 * â”‚ â””â”€ MobileNet-v3 Vision (async, ~16ms @ 60 FPS)               â”‚
 * â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 *
 * â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 * â”‚ CPU Cores 0-3 (Efficiency @ 2.3GHz)                          â”‚
 * â”‚ â”œâ”€ Mistral-7B Decode (streaming, 25-35 t/s)                  â”‚
 * â”‚ â””â”€ BGE Embeddings (async, ~500 emb/s)                        â”‚
 * â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 *
 * Concurrency Features:
 * - All models run in parallel without conflicts
 * - NPU and CPU workloads are independent
 * - Memory budget monitoring (~4.5GB total)
 * - Graceful degradation on resource constraints
 * - Async/await coordination for optimal performance
 */
class ConcurrentExecutionManager(private val context: Context) {

    // Model engines
    private val llmEngine = LLMInferenceEngine()
    private val visionManager = VisionManager(context)
    private val embeddingManager = EmbeddingManager()
    private val deviceAllocator = DeviceAllocationManager()

    // State tracking
    private val _isLLMBusy = MutableStateFlow(false)
    val isLLMBusy: StateFlow<Boolean> = _isLLMBusy.asStateFlow()

    private val _isVisionBusy = MutableStateFlow(false)
    val isVisionBusy: StateFlow<Boolean> = _isVisionBusy.asStateFlow()

    private val _isEmbeddingBusy = MutableStateFlow(false)
    val isEmbeddingBusy: StateFlow<Boolean> = _isEmbeddingBusy.asStateFlow()

    private val _systemStatus = MutableStateFlow<SystemStatus>(SystemStatus.Idle)
    val systemStatus: StateFlow<SystemStatus> = _systemStatus.asStateFlow()

    // Performance tracking
    data class PerformanceMetrics(
        val llmPrefillTimeMs: Long = 0,
        val llmDecodeTimeMs: Long = 0,
        val llmTokensPerSec: Float = 0f,
        val visionInferenceTimeMs: Long = 0,
        val visionFPS: Float = 0f,
        val embeddingThroughput: Float = 0f,
        val totalMemoryMB: Int = 0,
        val concurrentTasksActive: Int = 0
    )

    private val _performanceMetrics = MutableStateFlow(PerformanceMetrics())
    val performanceMetrics: StateFlow<PerformanceMetrics> = _performanceMetrics.asStateFlow()

    sealed class SystemStatus {
        object Idle : SystemStatus()
        object Initializing : SystemStatus()
        object Ready : SystemStatus()
        data class Error(val message: String) : SystemStatus()
    }

    /**
     * Initialize all models in production mode (NPU + CPU)
     */
    suspend fun initializeProductionMode(
        mistralModelFile: File,
        mobileNetModelFile: File,
        bgeModelFile: File
    ): Boolean = withContext(Dispatchers.IO) {
        try {
            _systemStatus.value = SystemStatus.Initializing

            Timber.i("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
            Timber.i("AI-Ish Concurrent Execution Manager")
            Timber.i("Initializing PRODUCTION MODE (S24 Ultra)")
            Timber.i("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")

            // Check memory budget
            val memoryOK = deviceAllocator.checkConcurrentMemoryBudget()
            if (!memoryOK) {
                val error = "Memory budget exceeded for concurrent execution"
                Timber.e(error)
                _systemStatus.value = SystemStatus.Error(error)
                return@withContext false
            }

            // Initialize all models concurrently
            val results = awaitAll(
                async { initializeLLM(mistralModelFile) },
                async { initializeVision(mobileNetModelFile) },
                async { initializeEmbedding(bgeModelFile) }
            )

            val allSuccess = results.all { it }

            if (allSuccess) {
                _systemStatus.value = SystemStatus.Ready
                Timber.i("âœ… All models initialized successfully")
                Timber.i(deviceAllocator.getAllocationSummary())
                Timber.i("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
                Timber.i("ğŸš€ CONCURRENT EXECUTION READY")
                Timber.i("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
            } else {
                val error = "Failed to initialize one or more models"
                Timber.e(error)
                _systemStatus.value = SystemStatus.Error(error)
            }

            allSuccess

        } catch (e: Exception) {
            Timber.e(e, "Error initializing concurrent execution manager")
            _systemStatus.value = SystemStatus.Error(e.message ?: "Unknown error")
            false
        }
    }

    /**
     * Initialize Mistral-7B INT8 (NPU prefill + CPU decode)
     */
    private suspend fun initializeLLM(modelFile: File): Boolean {
        return try {
            Timber.i("Initializing LLM (Mistral-7B INT8)...")
            llmEngine.loadModelProduction(modelFile)
        } catch (e: Exception) {
            Timber.e(e, "Error initializing LLM")
            false
        }
    }

    /**
     * Initialize MobileNet-v3 INT8 (NPU)
     */
    private suspend fun initializeVision(modelFile: File): Boolean {
        return try {
            Timber.i("Initializing Vision (MobileNet-v3 INT8)...")
            visionManager.initializeProduction()
        } catch (e: Exception) {
            Timber.e(e, "Error initializing Vision")
            false
        }
    }

    /**
     * Initialize BGE embeddings (CPU cores 0-3)
     */
    private suspend fun initializeEmbedding(modelFile: File): Boolean {
        return try {
            Timber.i("Initializing Embeddings (BGE-Small INT8)...")
            embeddingManager.loadModel(modelFile)
        } catch (e: Exception) {
            Timber.e(e, "Error initializing Embeddings")
            false
        }
    }

    /**
     * Execute LLM generation (NPU prefill + CPU decode streaming)
     * Can run concurrently with vision and embedding tasks
     */
    fun generateText(
        prompt: String,
        maxTokens: Int = 512,
        temperature: Float = 0.7f,
        topP: Float = 0.9f
    ): Flow<String> {
        _isLLMBusy.value = true
        updateConcurrentTaskCount()

        val flow = llmEngine.generateStream(
            prompt = prompt,
            maxTokens = maxTokens,
            temperature = temperature,
            topP = topP
        )

        // Update metrics after generation completes
        CoroutineScope(Dispatchers.Default).launch {
            flow.collect { }  // Consume flow
            updateLLMMetrics()
            _isLLMBusy.value = false
            updateConcurrentTaskCount()
        }

        return flow
    }

    /**
     * Execute vision inference (MobileNet-v3 on NPU)
     * Can run concurrently with LLM and embedding tasks
     */
    suspend fun analyzeImage(bitmap: Bitmap) = coroutineScope {
        _isVisionBusy.value = true
        updateConcurrentTaskCount()

        try {
            val result = visionManager.analyzeImage(bitmap)
            updateVisionMetrics()
            result
        } finally {
            _isVisionBusy.value = false
            updateConcurrentTaskCount()
        }
    }

    /**
     * Generate embeddings (BGE on CPU cores 0-3)
     * Can run concurrently with LLM and vision tasks
     */
    suspend fun generateEmbedding(text: String): FloatArray? = coroutineScope {
        _isEmbeddingBusy.value = true
        updateConcurrentTaskCount()

        try {
            val result = embeddingManager.generateEmbedding(text)
            updateEmbeddingMetrics()
            result
        } finally {
            _isEmbeddingBusy.value = false
            updateConcurrentTaskCount()
        }
    }

    /**
     * Generate batch of embeddings (BGE on CPU cores 0-3)
     */
    suspend fun generateEmbeddingsBatch(texts: List<String>): List<FloatArray> = coroutineScope {
        _isEmbeddingBusy.value = true
        updateConcurrentTaskCount()

        try {
            val result = embeddingManager.generateEmbeddingsBatch(texts)
            updateEmbeddingMetrics()
            result
        } finally {
            _isEmbeddingBusy.value = false
            updateConcurrentTaskCount()
        }
    }

    /**
     * Execute concurrent tasks: LLM + Vision + Embedding
     * Demonstrates true parallel execution on S24 Ultra
     */
    suspend fun executeConcurrentDemo(
        prompt: String,
        imageBitmap: Bitmap,
        embeddingText: String
    ): ConcurrentDemoResult = coroutineScope {
        Timber.i("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
        Timber.i("CONCURRENT EXECUTION DEMO")
        Timber.i("Running LLM + Vision + Embedding in parallel...")
        Timber.i("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")

        val startTime = System.currentTimeMillis()

        // Launch all three tasks in parallel
        val llmDeferred = async {
            val tokens = mutableListOf<String>()
            generateText(prompt, maxTokens = 100).collect { tokens.add(it) }
            tokens.joinToString("")
        }

        val visionDeferred = async {
            analyzeImage(imageBitmap)
        }

        val embeddingDeferred = async {
            generateEmbedding(embeddingText)
        }

        // Wait for all tasks to complete
        val llmResult = llmDeferred.await()
        val visionResult = visionDeferred.await()
        val embeddingResult = embeddingDeferred.await()

        val totalTime = System.currentTimeMillis() - startTime

        Timber.i("âœ… Concurrent execution complete: ${totalTime}ms")
        Timber.i("   - LLM: ${llmEngine.getLastPrefillTimeMs() + llmEngine.getLastDecodeTimeMs()}ms")
        Timber.i("   - Vision: ${visionManager.getLastInferenceTimeMs()}ms")
        Timber.i("   - Embedding: ~${embeddingResult?.size ?: 0}D vector")
        Timber.i("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")

        ConcurrentDemoResult(
            llmOutput = llmResult,
            visionOutput = visionResult.description,
            embeddingOutput = embeddingResult,
            totalTimeMs = totalTime
        )
    }

    data class ConcurrentDemoResult(
        val llmOutput: String,
        val visionOutput: String,
        val embeddingOutput: FloatArray?,
        val totalTimeMs: Long
    )

    /**
     * Update performance metrics
     */
    private fun updateLLMMetrics() {
        _performanceMetrics.value = _performanceMetrics.value.copy(
            llmPrefillTimeMs = llmEngine.getLastPrefillTimeMs(),
            llmDecodeTimeMs = llmEngine.getLastDecodeTimeMs(),
            llmTokensPerSec = llmEngine.getTokensPerSecond()
        )
    }

    private fun updateVisionMetrics() {
        _performanceMetrics.value = _performanceMetrics.value.copy(
            visionInferenceTimeMs = visionManager.getLastInferenceTimeMs(),
            visionFPS = visionManager.getLastFPS()
        )
    }

    private fun updateEmbeddingMetrics() {
        // BGE throughput is tracked internally by EmbeddingManager
        _performanceMetrics.value = _performanceMetrics.value.copy(
            embeddingThroughput = 500f  // Approximate from spec
        )
    }

    private fun updateConcurrentTaskCount() {
        val count = listOf(_isLLMBusy.value, _isVisionBusy.value, _isEmbeddingBusy.value)
            .count { it }

        _performanceMetrics.value = _performanceMetrics.value.copy(
            concurrentTasksActive = count
        )
    }

    /**
     * Get comprehensive system status
     */
    fun getSystemStatus(): String {
        val llmStatus = if (llmEngine.isLoaded()) {
            "âœ… ${llmEngine.getPerformanceMode()}"
        } else {
            "âŒ Not loaded"
        }

        val visionStatus = if (visionManager.isAvailable()) {
            "âœ… ${if (visionManager.isProductionMode()) "Production (NPU)" else "Legacy"}"
        } else {
            "âŒ Not loaded"
        }

        val embeddingStatus = if (embeddingManager.isLoaded()) {
            "âœ… CPU cores 0-3"
        } else {
            "âŒ Not loaded"
        }

        val metrics = _performanceMetrics.value

        return """
        |â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        | AI-Ish System Status (Samsung S24 Ultra)
        |â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        |
        | Models:
        |   â”œâ”€ LLM (Mistral-7B): $llmStatus
        |   â”œâ”€ Vision (MobileNet-v3): $visionStatus
        |   â””â”€ Embedding (BGE): $embeddingStatus
        |
        | Performance:
        |   â”œâ”€ LLM Prefill: ${metrics.llmPrefillTimeMs}ms (NPU)
        |   â”œâ”€ LLM Decode: ${metrics.llmTokensPerSec.toInt()} t/s (CPU)
        |   â”œâ”€ Vision: ${metrics.visionFPS.toInt()} FPS (NPU)
        |   â””â”€ Embedding: ${metrics.embeddingThroughput.toInt()} emb/s (CPU)
        |
        | Concurrent Tasks: ${metrics.concurrentTasksActive}
        |
        | System: ${_systemStatus.value}
        |â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        """.trimMargin()
    }

    /**
     * Release all resources
     */
    fun release() {
        Timber.i("Releasing concurrent execution manager...")
        llmEngine.release()
        visionManager.release()
        embeddingManager.release()
        _systemStatus.value = SystemStatus.Idle
        Timber.i("All resources released")
    }
}
